## üß™ **TODO - Tests d'autodiagnostic pour Jupiter**

L‚Äôobjectif de ces tests est de v√©rifier la capacit√© de Jupiter √† se **tester lui-m√™me** tout en validant ses fonctionnalit√©s de mani√®re coh√©rente. Ces tests doivent √™tre effectu√©s sur le projet Jupiter lui-m√™me, avec comparaison des r√©sultats pour garantir la stabilit√© et la performance des nouvelles fonctionnalit√©s.

### **1. Tests Unitaires & d'Int√©gration**

#### **1.1. Tests du module de scan (scan.py)**

* [ ] **Test du scan incr√©mental** :

  * V√©rifier que le scan incr√©mental fonctionne correctement sur des projets de tailles vari√©es.
  * Tester la gestion du cache (`.jupiter/cache/`) avec des changements mineurs.
  * Tester les options de filtre (fichiers cach√©s, `--ignore`, `--no-cache`).
* [ ] **Test de gestion des exclusions** :

  * V√©rifier que les fichiers et r√©pertoires exclus par `.jupiterignore` sont correctement ignor√©s.
  * Tester avec des glob patterns complexes.

#### **1.2. Tests du module d‚Äôanalyse (analyzer.py)**

* [ ] **Test d‚Äôanalyse dynamique** :

  * V√©rifier l‚Äôint√©gration de l‚Äôanalyse dynamique (fonctionnalit√©s appel√©es via `run` avec `--with-dynamic`).
  * V√©rifier que les m√©triques de performance (temps d'ex√©cution, appels de fonction) sont bien collect√©es et stock√©es dans le rapport dynamique.
* [ ] **Test de l‚Äôanalyse de code Python et JS/TS** :

  * V√©rifier que les statistiques d‚Äôanalyse (complexit√© cyclomatique, duplication de code) sont bien g√©n√©r√©es pour les fichiers Python, JS et TS.

#### **1.3. Tests du syst√®me de simulation (simulate.py)**

* [ ] **Simulation de suppression de fichier/fonction** :

  * Tester le comportement de la commande `simulate remove` pour divers types de fichiers et fonctions.
  * V√©rifier que le rapport d'impact est d√©taill√© et que les erreurs de lien sont correctement d√©tect√©es.

#### **1.4. Tests des snapshots et de l‚Äôhistorique (history.py)**

* [ ] **Cr√©ation et r√©cup√©ration de snapshots** :

  * V√©rifier la persistance des snapshots dans `.jupiter/snapshots/` apr√®s chaque scan.
  * Tester la fonctionnalit√© de `diff` entre deux snapshots pour les fichiers ajout√©s, supprim√©s, et modifi√©s.
  * V√©rifier que la version du sch√©ma est correctement ajout√©e dans chaque snapshot.

---

### **2. Tests d'API & WebUI**

#### **2.1. Tests des endpoints API**

* [x] **Test du endpoint `/scan`** :

  * V√©rifier que la commande `POST /scan` fonctionne correctement avec des options telles que `--incremental`, `--ignore`, et `--no-snapshot`.
* [x] **Test de l'endpoint `/analyze`** :

  * V√©rifier que l'analyse fonctionne correctement avec des projets de taille moyenne et grande, et que le format du rapport est coh√©rent.
* [x] **Test de la simulation via API (`/simulate/remove`)** :

  * V√©rifier que l‚Äôendpoint `/simulate/remove` renvoie des r√©sultats pr√©cis pour les fichiers et fonctions sp√©cifi√©s.
* [x] **Test de la gestion des snapshots via API** :

  * V√©rifier les endpoints `/snapshots` et `/snapshots/diff` pour la gestion des snapshots historiques et la comparaison des versions.

#### **2.2. Tests de la WebUI**

* [x] **Test de l'int√©gration WebUI avec l'API** :

  * V√©rifier que la WebUI est bien reli√©e √† l'API pour les commandes de scan, analyse, et simulation.
  * V√©rifier l‚Äôaffichage des r√©sultats dans le tableau de bord (status badges, derniers scans, etc.).
* [x] **Test des vues de gestion de projet (Backend)** :

  * V√©rifier la possibilit√© de s√©lectionner diff√©rents backends (local et distant) depuis l‚ÄôUI et tester les interactions avec les API distantes.
* [x] **Test des vues Snapshot & Diff** :

  * V√©rifier l'affichage des snapshots dans l'UI et tester la fonctionnalit√© de "diff" entre deux snapshots.
* [x] **Test du module Live Map** :

  * V√©rifier que le graphe interactif (Live Map) fonctionne correctement et s'adapte bien aux projets de diff√©rentes tailles.
  * V√©rifier que les n≈ìuds sont bien color√©s en fonction des m√©triques et du temps d'ex√©cution.

---

### **3. Tests de Performance & Stabilit√©**

#### **3.1. Tests de performance sur des projets volumineux**

* [ ] **Test sur un projet de grande taille** :

  * V√©rifier la stabilit√© et la performance de Jupiter lorsqu'il analyse un projet volumineux (plusieurs milliers de fichiers).
* [ ] **Tests de parall√©lisation du scan** :

  * V√©rifier que la parall√©lisation du scan fonctionne correctement en utilisant plusieurs threads pour les fichiers ind√©pendants.

#### **3.2. Tests de performance sur la Live Map**

* [ ] **Test de performance de la g√©n√©ration du graphe** :

  * Tester la g√©n√©ration du graphe de d√©pendances et l‚Äôaffichage dans la WebUI pour des projets larges, et s‚Äôassurer que les performances sont correctes.

---

### **4. Tests d‚ÄôInt√©gration CI/CD**

#### **4.1. Tests d‚Äôint√©gration dans un pipeline CI**

* [ ] **Tests avec GitHub Actions** :

  * V√©rifier l‚Äôint√©gration de Jupiter dans un pipeline CI/CD via GitHub Actions, en ex√©cutant un scan et une analyse, et en v√©rifiant que les **quality gates** sont bien appliqu√©s.
* [ ] **Tests avec d‚Äôautres CI (GitLab, etc.)** :

  * V√©rifier que les tests de Jupiter fonctionnent correctement dans un environnement Docker minimal pour CI.

---

### **5. Comparaison des R√©sultats Autodiagnostiqu√©s**

#### **5.1. Comparaison avec des tests classiques**

* [ ] **V√©rifier la coh√©rence des r√©sultats** :

  * Comparer les r√©sultats des tests effectu√©s par Jupiter avec ceux des tests classiques, pour s‚Äôassurer qu‚Äôil est capable de se diagnostiquer correctement.

#### **5.2. Validation de l‚Äôauto-diagnostic**

* [ ] **Tester les alertes sur code obsol√®te** :

  * V√©rifier que Jupiter d√©tecte correctement les fonctions et fichiers inutilis√©s ou obsol√®tes dans son propre code, et qu'il g√©n√®re des recommandations de refactorisation ou de nettoyage.

---

### **6. Mise √† jour de la documentation**

* [ ] **Mettre √† jour la documentation des tests** dans `dev_guide.md` et `user_guide.md` pour int√©grer toutes les nouvelles fonctionnalit√©s test√©es.
* [ ] **Mettre √† jour les changelogs** avec les nouvelles √©tapes de tests ajout√©es, les cas de tests r√©ussis, et les r√©sultats obtenus.

